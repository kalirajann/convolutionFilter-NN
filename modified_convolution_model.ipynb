{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¯ Model 2: Modified Convolution Model (Stride = 1)\n",
    "\n",
    "Based on **Generative Deep Learning (2nd Edition)**, Chapter 2 - Convolutions\n",
    "\n",
    "This notebook implements Model 2: a modified version with stride = 1. This model is identical to Model 1 in architecture but explicitly uses stride = 1 for the convolution layer to demonstrate the effect of stride on feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare the Dataset\n",
    "\n",
    "Using MNIST dataset (handwritten digits) - same as Model 1. Images are 28x28 grayscale, 10 classes (digits 0-9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING DATASET\n",
      "======================================================================\n",
      "Training data shape: (60000, 28, 28, 1)\n",
      "Test data shape: (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING DATASET\")\n",
    "print(\"=\" * 70)\n",
    "(x_train_full, y_train_full), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to [0, 1] range for better training stability\n",
    "x_train_full = x_train_full.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Expand dimensions to add channel axis: (60000, 28, 28) -> (60000, 28, 28, 1)\n",
    "# Conv2D layers require input shape: (height, width, channels)\n",
    "x_train_full = np.expand_dims(x_train_full, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "print(f\"Training data shape: {x_train_full.shape}\")\n",
    "print(f\"Test data shape: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train / Validation Split\n",
    "\n",
    "Same split as Model 1: 50,000 samples for training, 10,000 for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: 50000 samples\n",
      "Validation set: 10000 samples\n",
      "Test set: 10000 samples\n"
     ]
    }
   ],
   "source": [
    "x_train, x_val = x_train_full[:50000], x_train_full[50000:]\n",
    "y_train, y_val = y_train_full[:50000], y_train_full[50000:]\n",
    "\n",
    "print(f\"\\nTrain set: {x_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {x_val.shape[0]} samples\")\n",
    "print(f\"Test set: {x_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build the Modified Convolution Model (Stride = 1)\n",
    "\n",
    "**KEY MODIFICATION: stride = (1, 1)**\n",
    "\n",
    "- Stride = 1 means the filter moves 1 pixel at a time in both directions\n",
    "- This preserves maximum spatial information from the input\n",
    "- With stride = 1 and valid padding, the output feature map size is: `output_size = (input_size - kernel_size + 1) / stride`\n",
    "- For 28x28 input with 3x3 kernel: (28 - 3 + 1) / 1 = 26x26\n",
    "\n",
    "**Why stride = 1 preserves more spatial information:**\n",
    "- The filter examines every possible position in the input image\n",
    "- No information is skipped between positions\n",
    "- This results in a larger feature map (26x26) compared to stride = 2 (13x13)\n",
    "- More spatial detail is retained, which can help with fine-grained features\n",
    "\n",
    "**Comparison to Model 1:**\n",
    "- Model 1 also used stride = 1, so this model is architecturally identical\n",
    "- However, explicitly setting stride = 1 demonstrates the baseline case\n",
    "- If stride were 2, the feature map would be 13x13, losing spatial resolution\n",
    "- Stride = 1 is optimal for preserving spatial relationships in images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BUILDING MODEL 2: MODIFIED CONVOLUTION (STRIDE = 1)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"BUILDING MODEL 2: MODIFIED CONVOLUTION (STRIDE = 1)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "# Input layer (Keras 3.x recommended approach to avoid warnings)\n",
    "model.add(layers.Input(shape=(28, 28, 1)))\n",
    "\n",
    "# Convolution layer with stride = 1 (explicitly set)\n",
    "model.add(layers.Conv2D(filters=32,\n",
    "                        kernel_size=(3, 3),\n",
    "                        strides=(1, 1),  # EXPLICITLY SET TO 1\n",
    "                        padding='valid',\n",
    "                        activation='relu'))\n",
    "\n",
    "# Flatten the 2D feature maps into a 1D vector for dense layers\n",
    "# Output from Conv2D: (batch, 26, 26, 32) -> Flatten -> (batch, 26*26*32)\n",
    "# With stride = 1, we get 26x26 = 676 spatial positions per filter\n",
    "# Total flattened size: 26 * 26 * 32 = 21,632 features\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Dense (fully connected) layer for feature combination\n",
    "# 32 units: same as Model 1 for direct comparison\n",
    "model.add(layers.Dense(units=32, activation='relu'))\n",
    "\n",
    "# Output layer: 10 units for 10 digit classes, softmax for probability distribution\n",
    "model.add(layers.Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compile the Model\n",
    "\n",
    "Same optimizer and loss as Model 1 for direct comparison. Using RMSprop optimizer and sparse categorical crossentropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.RMSprop(),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Summary and Convolution Layer Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MODEL SUMMARY\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21632</span>)          â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">692,256</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚           \u001b[38;5;34m320\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ flatten (\u001b[38;5;33mFlatten\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21632\u001b[0m)          â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚       \u001b[38;5;34m692,256\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             â”‚           \u001b[38;5;34m330\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">692,906</span> (2.64 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m692,906\u001b[0m (2.64 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">692,906</span> (2.64 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m692,906\u001b[0m (2.64 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CONVOLUTION LAYER CONFIGURATION\n",
      "======================================================================\n",
      "Layer Name: conv2d\n",
      "  Filters: 32\n",
      "  Kernel Size: (3, 3)\n",
      "  Strides: (1, 1)  <-- EXPLICITLY SET TO (1, 1)\n",
      "  Padding: valid\n",
      "  Activation: relu\n",
      "  Input Shape: (28, 28, 1)\n",
      "  Output Shape: (26, 26, 32)  [28-3+1=26 with stride=1, valid padding, 32 filters]\n",
      "\n",
      "  NOTE: Stride = 1 means the filter examines every pixel position,\n",
      "        preserving maximum spatial information in the feature map.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"MODEL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "model.summary()\n",
    "\n",
    "# Extract and display convolution layer configuration\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CONVOLUTION LAYER CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, layers.Conv2D):\n",
    "        print(f\"Layer Name: {layer.name}\")\n",
    "        print(f\"  Filters: {layer.filters}\")\n",
    "        print(f\"  Kernel Size: {layer.kernel_size}\")\n",
    "        print(f\"  Strides: {layer.strides}  <-- EXPLICITLY SET TO (1, 1)\")\n",
    "        print(f\"  Padding: {layer.padding}\")\n",
    "        # Get activation name safely\n",
    "        if hasattr(layer.activation, '__name__'):\n",
    "            activation_name = layer.activation.__name__\n",
    "        elif callable(layer.activation):\n",
    "            activation_name = str(layer.activation)\n",
    "        else:\n",
    "            activation_name = str(layer.activation)\n",
    "        print(f\"  Activation: {activation_name}\")\n",
    "        print(f\"  Input Shape: (28, 28, 1)\")\n",
    "        print(f\"  Output Shape: (26, 26, 32)  [28-3+1=26 with stride=1, valid padding, 32 filters]\")\n",
    "        print(f\"\\n  NOTE: Stride = 1 means the filter examines every pixel position,\")\n",
    "        print(f\"        preserving maximum spatial information in the feature map.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the Model\n",
    "\n",
    "Training with 5 epochs, batch size 64, RMSprop optimizer, and sparse categorical crossentropy loss. **Key modification: Convolution Stride = (1, 1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TRAINING MODEL 2: MODIFIED CONVOLUTION (STRIDE = 1)\n",
      "======================================================================\n",
      "Training with the following settings:\n",
      "  - Epochs: 5\n",
      "  - Batch Size: 64\n",
      "  - Optimizer: RMSprop\n",
      "  - Loss: Sparse Categorical Crossentropy\n",
      "  - Convolution Stride: (1, 1)  <-- KEY MODIFICATION\n",
      "======================================================================\n",
      "Epoch 1/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9332 - loss: 0.2241 - val_accuracy: 0.9727 - val_loss: 0.0943\n",
      "Epoch 2/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9777 - loss: 0.0738 - val_accuracy: 0.9793 - val_loss: 0.0714\n",
      "Epoch 3/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9851 - loss: 0.0485 - val_accuracy: 0.9796 - val_loss: 0.0685\n",
      "Epoch 4/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9892 - loss: 0.0349 - val_accuracy: 0.9813 - val_loss: 0.0692\n",
      "Epoch 5/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9927 - loss: 0.0254 - val_accuracy: 0.9818 - val_loss: 0.0737\n",
      "\n",
      "======================================================================\n",
      "TRAINING HISTORY - ACCURACY PER EPOCH\n",
      "======================================================================\n",
      "Epoch 1:\n",
      "  Training Accuracy:   0.9332\n",
      "  Validation Accuracy: 0.9727\n",
      "Epoch 2:\n",
      "  Training Accuracy:   0.9777\n",
      "  Validation Accuracy: 0.9793\n",
      "Epoch 3:\n",
      "  Training Accuracy:   0.9851\n",
      "  Validation Accuracy: 0.9796\n",
      "Epoch 4:\n",
      "  Training Accuracy:   0.9892\n",
      "  Validation Accuracy: 0.9813\n",
      "Epoch 5:\n",
      "  Training Accuracy:   0.9927\n",
      "  Validation Accuracy: 0.9818\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TRAINING MODEL 2: MODIFIED CONVOLUTION (STRIDE = 1)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Training with the following settings:\")\n",
    "print(\"  - Epochs: 5\")\n",
    "print(\"  - Batch Size: 64\")\n",
    "print(\"  - Optimizer: RMSprop\")\n",
    "print(\"  - Loss: Sparse Categorical Crossentropy\")\n",
    "print(\"  - Convolution Stride: (1, 1)  <-- KEY MODIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=5,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)\n",
    "\n",
    "# Extract final training and validation accuracies\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "final_val_acc = history.history['val_accuracy'][-1]\n",
    "\n",
    "# Print per-epoch accuracies\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING HISTORY - ACCURACY PER EPOCH\")\n",
    "print(\"=\" * 70)\n",
    "for epoch in range(len(history.history['accuracy'])):\n",
    "    print(f\"Epoch {epoch + 1}:\")\n",
    "    print(f\"  Training Accuracy:   {history.history['accuracy'][epoch]:.4f}\")\n",
    "    print(f\"  Validation Accuracy: {history.history['val_accuracy'][epoch]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EVALUATING ON TEST SET\n",
      "======================================================================\n",
      "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 837us/step - accuracy: 0.9824 - loss: 0.0628\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"EVALUATING ON TEST SET\")\n",
    "print(\"=\" * 70)\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Results (Formatted for Academic Submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MODEL 2: MODIFIED CONVOLUTION (STRIDE = 1)\n",
      "======================================================================\n",
      "Final Training Accuracy:   0.9927\n",
      "Final Validation Accuracy: 0.9818\n",
      "Final Test Accuracy:       0.9824\n",
      "Final Test Loss:           0.0628\n",
      "======================================================================\n",
      "\n",
      "Model Configuration Summary:\n",
      "  - Architecture: Conv2D -> Flatten -> Dense -> Dense\n",
      "  - Conv2D: 32 filters, 3x3 kernel, stride = (1, 1), valid padding\n",
      "  - Stride = 1 preserves maximum spatial resolution (26x26 feature maps)\n",
      "  - This model is architecturally identical to Model 1\n",
      "  - Stride = 1 allows the filter to examine every pixel position\n",
      "  - No pooling, no batch normalization, minimal architecture\n",
      "======================================================================\n",
      "\n",
      "Comparison Notes:\n",
      "  - Stride = 1 means the convolution filter moves 1 pixel at a time\n",
      "  - This preserves more spatial information than larger strides\n",
      "  - Feature map size: 26x26 (with 3x3 kernel and valid padding)\n",
      "  - If stride were 2, feature map would be 13x13, losing spatial detail\n",
      "  - Stride = 1 is optimal for preserving fine-grained image features\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL 2: MODIFIED CONVOLUTION (STRIDE = 1)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Final Training Accuracy:   {final_train_acc:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {final_val_acc:.4f}\")\n",
    "print(f\"Final Test Accuracy:       {test_acc:.4f}\")\n",
    "print(f\"Final Test Loss:           {test_loss:.4f}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nModel Configuration Summary:\")\n",
    "print(\"  - Architecture: Conv2D -> Flatten -> Dense -> Dense\")\n",
    "print(\"  - Conv2D: 32 filters, 3x3 kernel, stride = (1, 1), valid padding\")\n",
    "print(\"  - Stride = 1 preserves maximum spatial resolution (26x26 feature maps)\")\n",
    "print(\"  - This model is architecturally identical to Model 1\")\n",
    "print(\"  - Stride = 1 allows the filter to examine every pixel position\")\n",
    "print(\"  - No pooling, no batch normalization, minimal architecture\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nComparison Notes:\")\n",
    "print(\"  - Stride = 1 means the convolution filter moves 1 pixel at a time\")\n",
    "print(\"  - This preserves more spatial information than larger strides\")\n",
    "print(\"  - Feature map size: 26x26 (with 3x3 kernel and valid padding)\")\n",
    "print(\"  - If stride were 2, feature map would be 13x13, losing spatial detail\")\n",
    "print(\"  - Stride = 1 is optimal for preserving fine-grained image features\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (convfilter)",
   "language": "python",
   "name": "convfilter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
