{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸŽ¯ Model 2: Modified Convolution Model (Stride = 1)\n",
        "\n",
        "Based on **Generative Deep Learning (2nd Edition)**, Chapter 2 - Convolutions\n",
        "\n",
        "This notebook implements Model 2: a modified version with stride = 1. This model is identical to Model 1 in architecture but explicitly uses stride = 1 for the convolution layer to demonstrate the effect of stride on feature maps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Prepare the Dataset\n",
        "\n",
        "Using MNIST dataset (handwritten digits) - same as Model 1. Images are 28x28 grayscale, 10 classes (digits 0-9)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load MNIST dataset\n",
        "print(\"=\" * 70)\n",
        "print(\"LOADING DATASET\")\n",
        "print(\"=\" * 70)\n",
        "(x_train_full, y_train_full), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to [0, 1] range for better training stability\n",
        "x_train_full = x_train_full.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Expand dimensions to add channel axis: (60000, 28, 28) -> (60000, 28, 28, 1)\n",
        "# Conv2D layers require input shape: (height, width, channels)\n",
        "x_train_full = np.expand_dims(x_train_full, axis=-1)\n",
        "x_test = np.expand_dims(x_test, axis=-1)\n",
        "\n",
        "print(f\"Training data shape: {x_train_full.shape}\")\n",
        "print(f\"Test data shape: {x_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Train / Validation Split\n",
        "\n",
        "Same split as Model 1: 50,000 samples for training, 10,000 for validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train, x_val = x_train_full[:50000], x_train_full[50000:]\n",
        "y_train, y_val = y_train_full[:50000], y_train_full[50000:]\n",
        "\n",
        "print(f\"\\nTrain set: {x_train.shape[0]} samples\")\n",
        "print(f\"Validation set: {x_val.shape[0]} samples\")\n",
        "print(f\"Test set: {x_test.shape[0]} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Build the Modified Convolution Model (Stride = 1)\n",
        "\n",
        "**KEY MODIFICATION: stride = (1, 1)**\n",
        "\n",
        "- Stride = 1 means the filter moves 1 pixel at a time in both directions\n",
        "- This preserves maximum spatial information from the input\n",
        "- With stride = 1 and valid padding, the output feature map size is: `output_size = (input_size - kernel_size + 1) / stride`\n",
        "- For 28x28 input with 3x3 kernel: (28 - 3 + 1) / 1 = 26x26\n",
        "\n",
        "**Why stride = 1 preserves more spatial information:**\n",
        "- The filter examines every possible position in the input image\n",
        "- No information is skipped between positions\n",
        "- This results in a larger feature map (26x26) compared to stride = 2 (13x13)\n",
        "- More spatial detail is retained, which can help with fine-grained features\n",
        "\n",
        "**Comparison to Model 1:**\n",
        "- Model 1 also used stride = 1, so this model is architecturally identical\n",
        "- However, explicitly setting stride = 1 demonstrates the baseline case\n",
        "- If stride were 2, the feature map would be 13x13, losing spatial resolution\n",
        "- Stride = 1 is optimal for preserving spatial relationships in images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"BUILDING MODEL 2: MODIFIED CONVOLUTION (STRIDE = 1)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "# Convolution layer with stride = 1 (explicitly set)\n",
        "model.add(layers.Conv2D(filters=32,\n",
        "                        kernel_size=(3, 3),\n",
        "                        strides=(1, 1),  # EXPLICITLY SET TO 1\n",
        "                        padding='valid',\n",
        "                        activation='relu',\n",
        "                        input_shape=(28, 28, 1)))\n",
        "\n",
        "# Flatten the 2D feature maps into a 1D vector for dense layers\n",
        "# Output from Conv2D: (batch, 26, 26, 32) -> Flatten -> (batch, 26*26*32)\n",
        "# With stride = 1, we get 26x26 = 676 spatial positions per filter\n",
        "# Total flattened size: 26 * 26 * 32 = 21,632 features\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "# Dense (fully connected) layer for feature combination\n",
        "# 32 units: same as Model 1 for direct comparison\n",
        "model.add(layers.Dense(units=32, activation='relu'))\n",
        "\n",
        "# Output layer: 10 units for 10 digit classes, softmax for probability distribution\n",
        "model.add(layers.Dense(units=10, activation='softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Compile the Model\n",
        "\n",
        "Same optimizer and loss as Model 1 for direct comparison. Using RMSprop optimizer and sparse categorical crossentropy loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(optimizer=optimizers.RMSprop(),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Summary and Convolution Layer Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"MODEL SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "model.summary()\n",
        "\n",
        "# Extract and display convolution layer configuration\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"CONVOLUTION LAYER CONFIGURATION\")\n",
        "print(\"=\" * 70)\n",
        "for layer in model.layers:\n",
        "    if isinstance(layer, layers.Conv2D):\n",
        "        print(f\"Layer Name: {layer.name}\")\n",
        "        print(f\"  Filters: {layer.filters}\")\n",
        "        print(f\"  Kernel Size: {layer.kernel_size}\")\n",
        "        print(f\"  Strides: {layer.strides}  <-- EXPLICITLY SET TO (1, 1)\")\n",
        "        print(f\"  Padding: {layer.padding}\")\n",
        "        # Get activation name safely\n",
        "        if hasattr(layer.activation, '__name__'):\n",
        "            activation_name = layer.activation.__name__\n",
        "        elif callable(layer.activation):\n",
        "            activation_name = str(layer.activation)\n",
        "        else:\n",
        "            activation_name = str(layer.activation)\n",
        "        print(f\"  Activation: {activation_name}\")\n",
        "        print(f\"  Input Shape: (28, 28, 1)\")\n",
        "        print(f\"  Output Shape: (26, 26, 32)  [28-3+1=26 with stride=1, valid padding, 32 filters]\")\n",
        "        print(f\"\\n  NOTE: Stride = 1 means the filter examines every pixel position,\")\n",
        "        print(f\"        preserving maximum spatial information in the feature map.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Train the Model\n",
        "\n",
        "Training with 5 epochs, batch size 64, RMSprop optimizer, and sparse categorical crossentropy loss. **Key modification: Convolution Stride = (1, 1)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"TRAINING MODEL 2: MODIFIED CONVOLUTION (STRIDE = 1)\")\n",
        "print(\"=\" * 70)\n",
        "print(\"Training with the following settings:\")\n",
        "print(\"  - Epochs: 5\")\n",
        "print(\"  - Batch Size: 64\")\n",
        "print(\"  - Optimizer: RMSprop\")\n",
        "print(\"  - Loss: Sparse Categorical Crossentropy\")\n",
        "print(\"  - Convolution Stride: (1, 1)  <-- KEY MODIFICATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=5,\n",
        "                    batch_size=64,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    verbose=1)\n",
        "\n",
        "# Extract final training and validation accuracies\n",
        "final_train_acc = history.history['accuracy'][-1]\n",
        "final_val_acc = history.history['val_accuracy'][-1]\n",
        "\n",
        "# Print per-epoch accuracies\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TRAINING HISTORY - ACCURACY PER EPOCH\")\n",
        "print(\"=\" * 70)\n",
        "for epoch in range(len(history.history['accuracy'])):\n",
        "    print(f\"Epoch {epoch + 1}:\")\n",
        "    print(f\"  Training Accuracy:   {history.history['accuracy'][epoch]:.4f}\")\n",
        "    print(f\"  Validation Accuracy: {history.history['val_accuracy'][epoch]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"EVALUATING ON TEST SET\")\n",
        "print(\"=\" * 70)\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Final Results (Formatted for Academic Submission)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"MODEL 2: MODIFIED CONVOLUTION (STRIDE = 1)\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Final Training Accuracy:   {final_train_acc:.4f}\")\n",
        "print(f\"Final Validation Accuracy: {final_val_acc:.4f}\")\n",
        "print(f\"Final Test Accuracy:       {test_acc:.4f}\")\n",
        "print(f\"Final Test Loss:           {test_loss:.4f}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nModel Configuration Summary:\")\n",
        "print(\"  - Architecture: Conv2D -> Flatten -> Dense -> Dense\")\n",
        "print(\"  - Conv2D: 32 filters, 3x3 kernel, stride = (1, 1), valid padding\")\n",
        "print(\"  - Stride = 1 preserves maximum spatial resolution (26x26 feature maps)\")\n",
        "print(\"  - This model is architecturally identical to Model 1\")\n",
        "print(\"  - Stride = 1 allows the filter to examine every pixel position\")\n",
        "print(\"  - No pooling, no batch normalization, minimal architecture\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nComparison Notes:\")\n",
        "print(\"  - Stride = 1 means the convolution filter moves 1 pixel at a time\")\n",
        "print(\"  - This preserves more spatial information than larger strides\")\n",
        "print(\"  - Feature map size: 26x26 (with 3x3 kernel and valid padding)\")\n",
        "print(\"  - If stride were 2, feature map would be 13x13, losing spatial detail\")\n",
        "print(\"  - Stride = 1 is optimal for preserving fine-grained image features\")\n",
        "print(\"=\" * 70)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
